{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "JDAgp_sTX_st"
      },
      "outputs": [],
      "source": [
        "######## Install Tensorflow version 1.13.2\n",
        "!pip install tensorflow==1.13.2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "LH2Ikk20YiH_"
      },
      "outputs": [],
      "source": [
        "######## Official git repo\n",
        "##\n",
        "##     https://www.github.com/purvesh-linux11/imageCaptioning.git\n",
        "##\n",
        "##### clone project moduls\n",
        "\n",
        "!git clone https://www.github.com/purvesh-linux11/imageCaptioning.git\n",
        "!cp -r /content/imageCaptioning/* /content/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "2AkEa0BupDsF"
      },
      "outputs": [],
      "source": [
        "##### copy model from drive and extract here\n",
        "\n",
        "!cp -r /content/drive/'My Drive'/MODELS/image_captioning_model.zip /content/  \n",
        "!unzip image_captioning_model.zip\n",
        "!cp -r /content/image_captioning_model/* /content/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "sYZGzNViwPQ1"
      },
      "outputs": [],
      "source": [
        "##### copy photos from drive\n",
        "\n",
        "!cp -r /content/drive/'My Drive'/photos/* /content/photos/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "xDwlCAOMRG5q"
      },
      "outputs": [],
      "source": [
        "#############################################################\n",
        "##\n",
        "##\n",
        "## Download Model and place it current directory \n",
        "##                  (Link in Readme file)\n",
        "##\n",
        "##############################################################"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "-ErYFuETaEGX"
      },
      "outputs": [
        {
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'tensorflow.contrib'",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "Cell \u001b[1;32mIn [1], line 23\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mio\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BytesIO\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mim2txt\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m configuration\n\u001b[1;32m---> 23\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mim2txt\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m inference_wrapper\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mim2txt\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01minference_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m caption_generator\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mim2txt\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01minference_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m vocabulary\n",
            "File \u001b[1;32mc:\\Users\\Admin\\Desktop\\Semester 5\\NLP\\imageCaptioning-master\\im2txt\\inference_wrapper.py:24\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m__future__\u001b[39;00m \u001b[39mimport\u001b[39;00m division\n\u001b[0;32m     20\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m__future__\u001b[39;00m \u001b[39mimport\u001b[39;00m print_function\n\u001b[1;32m---> 24\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mim2txt\u001b[39;00m \u001b[39mimport\u001b[39;00m show_and_tell_model\n\u001b[0;32m     25\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mim2txt\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39minference_utils\u001b[39;00m \u001b[39mimport\u001b[39;00m inference_wrapper_base\n\u001b[0;32m     28\u001b[0m \u001b[39mclass\u001b[39;00m \u001b[39mInferenceWrapper\u001b[39;00m(inference_wrapper_base\u001b[39m.\u001b[39mInferenceWrapperBase):\n",
            "File \u001b[1;32mc:\\Users\\Admin\\Desktop\\Semester 5\\NLP\\imageCaptioning-master\\im2txt\\show_and_tell_model.py:29\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m__future__\u001b[39;00m \u001b[39mimport\u001b[39;00m print_function\n\u001b[0;32m     27\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mtf\u001b[39;00m\n\u001b[1;32m---> 29\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mim2txt\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mops\u001b[39;00m \u001b[39mimport\u001b[39;00m image_embedding\n\u001b[0;32m     30\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mim2txt\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mops\u001b[39;00m \u001b[39mimport\u001b[39;00m image_processing\n\u001b[0;32m     31\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mim2txt\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mops\u001b[39;00m \u001b[39mimport\u001b[39;00m inputs \u001b[39mas\u001b[39;00m input_ops\n",
            "File \u001b[1;32mc:\\Users\\Admin\\Desktop\\Semester 5\\NLP\\imageCaptioning-master\\im2txt\\ops\\image_embedding.py:25\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m__future__\u001b[39;00m \u001b[39mimport\u001b[39;00m print_function\n\u001b[0;32m     23\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mtf\u001b[39;00m\n\u001b[1;32m---> 25\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcontrib\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mslim\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mslim\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mnets\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39minception_v3\u001b[39;00m \u001b[39mimport\u001b[39;00m inception_v3_base\n\u001b[0;32m     27\u001b[0m slim \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mcontrib\u001b[39m.\u001b[39mslim\n\u001b[0;32m     30\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39minception_v3\u001b[39m(images,\n\u001b[0;32m     31\u001b[0m                  trainable\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[0;32m     32\u001b[0m                  is_training\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     38\u001b[0m                  add_summaries\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[0;32m     39\u001b[0m                  scope\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mInceptionV3\u001b[39m\u001b[39m\"\u001b[39m):\n",
            "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow.contrib'"
          ]
        }
      ],
      "source": [
        "from __future__ import absolute_import\n",
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "\n",
        "import math\n",
        "import os\n",
        "from os import listdir, path\n",
        "\n",
        "import tensorflow as tf\n",
        "import cv2\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython import display\n",
        "\n",
        "import PIL.Image\n",
        "import PIL.ImageOps\n",
        "import PIL.ImageFont\n",
        "import PIL.ImageDraw\n",
        "import textwrap\n",
        "from io import BytesIO\n",
        "\n",
        "from im2txt import configuration\n",
        "from im2txt import inference_wrapper\n",
        "from im2txt.inference_utils import caption_generator\n",
        "from im2txt.inference_utils import vocabulary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "fN-tgbJMbNHt"
      },
      "outputs": [],
      "source": [
        "# Choose the trained model --> current is 2\n",
        "model_number = \"2\"\n",
        "model_path = \"/content/model.ckpt-\"+model_number+\"000000\"   # Give model path\n",
        "vocab_path = \"/content/word_counts\"+model_number+\".txt\"     # Give word_counts file path\n",
        "tf.logging.set_verbosity(tf.logging.INFO)\n",
        "\n",
        "# Build the inference graph.\n",
        "g = tf.Graph()\n",
        "with g.as_default():\n",
        "    model = inference_wrapper.InferenceWrapper()\n",
        "    restore_fn = model.build_graph_from_config(configuration.ModelConfig(), model_path)\n",
        "g.finalize()\n",
        "\n",
        "# Create the vocabulary.\n",
        "vocab = vocabulary.Vocabulary(vocab_path) \n",
        "\n",
        "#######################################################\n",
        "### if tensorflow version is <1.13.2 then you have to check variables name as per tensorflow version\n",
        "\n",
        "OLD_CHECKPOINT_FILE = \"/content/model.ckpt-2000000\"\n",
        "NEW_CHECKPOINT_FILE = \"/content/model.ckpt-2000000\"\n",
        "\n",
        "import tensorflow as tf\n",
        "vars_to_rename = {\n",
        "    \"lstm/BasicLSTMCell/Linear/Matrix\": \"lstm/basic_lstm_cell/kernel\",\n",
        "    \"lstm/BasicLSTMCell/Linear/Bias\": \"lstm/basic_lstm_cell/bias\",\n",
        "}\n",
        "new_checkpoint_vars = {}\n",
        "reader = tf.train.NewCheckpointReader(OLD_CHECKPOINT_FILE)\n",
        "for old_name in reader.get_variable_to_shape_map():\n",
        "  if old_name in vars_to_rename:\n",
        "    new_name = vars_to_rename[old_name]\n",
        "  else:\n",
        "    new_name = old_name\n",
        "  new_checkpoint_vars[new_name] = tf.Variable(reader.get_tensor(old_name))\n",
        "\n",
        "init = tf.global_variables_initializer()\n",
        "saver = tf.train.Saver(new_checkpoint_vars)\n",
        "\n",
        "with tf.Session() as sess:\n",
        "  sess.run(init)\n",
        "  saver.save(sess, NEW_CHECKPOINT_FILE)\n",
        "\n",
        "#######################################################\n",
        "\n",
        "sess = tf.Session(graph=g)\n",
        "# Load the model from checkpoint.\n",
        "restore_fn(sess)\n",
        "\n",
        "# Prepare the caption generator. Here we are implicitly using the default\n",
        "# beam search parameters. See caption_generator.py for a description of the\n",
        "# available beam search parameters.\n",
        "generator = caption_generator.CaptionGenerator(model, vocab)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "DIZLzNZIbgU-"
      },
      "outputs": [],
      "source": [
        "image_path = \"./photos/\"    #### provide path where image is stored\n",
        "filename = listdir(image_path)\n",
        "filenames = [f for f in filename if '.jpg' in f or '.png' in f or '.jpeg' in f]\n",
        "store = open('/content/results/logs.txt','w')   #### directory to store captions file\n",
        "\n",
        "for file in filenames:\n",
        "    try:\n",
        "        img = PIL.Image.open(image_path+file).convert('RGBA')\n",
        "        box = PIL.Image.new('RGBA', img.size, (255,255,255,0))\n",
        "        draw = PIL.ImageDraw.Draw(box)\n",
        "        image = open(image_path+file,'rb').read() # Read the image as bytes\n",
        "        captions = generator.beam_search(sess, image)\n",
        "        for i, caption in enumerate(captions):\n",
        "            # Ignore begin and end words.\n",
        "            sentence = [vocab.id_to_word(w) for w in caption.sentence[1:-1]]\n",
        "            sentence = \" \".join(sentence)\n",
        "            if i==0:\n",
        "              print(file+\" : %s \" % (sentence))\n",
        "              #create log file image_name--image_caption\n",
        "              store.write(file+\" : \"+sentence+\"\\n\")\n",
        "    except KeyboardInterrupt:\n",
        "        store.close()\n",
        "        break\n",
        "store.close()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "imageCaptioning.ipynb",
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.10.5 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.5"
    },
    "vscode": {
      "interpreter": {
        "hash": "a42ccb73e7d9bfdf27e036f1d2b8b681e55fc0743cc5586bc2474d4a60f4b886"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
